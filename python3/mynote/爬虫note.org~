* 目录
** headers的设置
#+BEGIN_SRC python
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64; rv:23.0) Gecko/20100101 Firefox/23.0','Referer':'http://xxxx.xxx.xxx'}
#+END_SRC

- User-Agent 有些服务器或 Proxy 会通过该值来判断是否是浏览器发出的请求是浏览器的信息
- Referer 用来防盗链

** proxy(代理)的设置

#+BEGIN_SRC python
  import urllib
  enable_proxy = True
  proxy_handler = urllib.request.ProxyHandler({"http" : 'http://some-proxy.com:8080'})
  null_proxy_handler = urllib.request.ProxyHandler({})
  if enable_proxy:
      opener = urllib.request.build_opener(proxy_handler)
  else:
      opener = urllib.request.build_opener(null_proxy_handler)
  urllib.request.install_opener(opener)
#+END_SRC

** Timeout的设置
#+BEGIN_SRC python
import urllib
response = urllib.request.urlopen('http://www.baidu.com', timeout=10)
#+END_SRC

#+BEGIN_SRC python
import urllib
response = urllib.resquest.urlopen('http://www.baidu.com',date,10)
#+END_SRC
   
** 使用http的put和delete方法
http协议有六种请求方法,get,head,put,post,delete,options,有时候我们需要put方式和delete方式请求。

PUT：这个方法比较少见。HTML表单也不支持这个。本质上来讲， PUT和POST极为相似，都是向服务器发送数据，但它们之间有一个重要区别，PUT通常指定了资源的存放位置，而POST则没有，POST的数据存放位置由服务器自己决定。

DELETE：删除某一个资源。基本上这个也很少见，不过还是有一些地方比如amazon的S3云服务里面就用的这个方法来删除资源。

#+BEGIN_SRC python
  import urllib
  request = request.Request(uri, data=data)
  request.get_method = lambda: 'PUT' # or 'DELETE'
  response = request.urlopen(request)
#+END_SRC

** 使用urllib.parse.urlencode()将字典转换成url形式的字符串
#+BEGIN_SRC python
  import http
  import urllib

  data = {'name':'yuanzishuai','age':23}
  url_data = urllib.parse.urlencode(data)

  print(url_data)
#+END_SRC

** 伪装成浏览器访问网站，两种方法。
*** 简便直接的方法
#+BEGIN_SRC python
  import urllib

  headers = {
      'Connection': 'Keep-Alive',
      'Accept': 'text/html, application/xhtml+xml, */*',
      'Accept-Language': 'en-US,en;q=0.8,zh-Hans-CN;q=0.5,zh-Hans;q=0.3',
      'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko'
  }
  url="http://www.zhihu.com"
  reg = urllib.request.Request(url,headers = headers)
  opener = urllib.request.urlopen(reg)
  content = opener.read()
  print(content.decode())
#+END_SRC

*** 使用request.build_opener()方法
#+BEGIN_SRC python
  import urllib
  import http.cookiejar
  def makeMyOpener(headers = {
      'Connection': 'Keep-Alive',
      'Accept': 'text/html, application/xhtml+xml, */*',
      'Accept-Language': 'en-US,en;q=0.8,zh-Hans-CN;q=0.5,zh-Hans;q=0.3',
      'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko'
  }):
      cookies = http.cookiejar.CookieJar()
      opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cookies))
      header = []
      for key,value in headers.items():
          elem = (key,value) #元组
          header.append(elem)
      opener.addheaders = header
      return opener


  url = "http://www.zhihu.com"
  opener = makeMyOpener() #class urllib.request.OpenerDirector
  content = opener.open(url)  # class http.client.HTTPResponse
  print(content.read().decode())
#+END_SRC
    
** 保存抓回来的报文
#+BEGIN_SRC python
  def saveFile(data):
      data_path = "d:/temp.out"
      f.obj = open(data_path , "wb")
      f.obj.write(data)
      f.obj.close()
#+END_SRC
** 通过cookie登录知乎
*** 第一步
使用fiddler观察浏览器的行为。用python把cookies的信息记录下来

*** 第二步
解压缩获取到的数据
#+BEGIN_SRC python
  import gzip
  def ungzip(data):
      try:        # 尝试解压
          print('正在解压.....')
          data = gzip.decompress(data)
          print('解压完毕!')
      except:
          print('未经压缩, 无需解压')
      return data
#+END_SRC

*** 第三步: 使用正则表达式获取沙漠之舟
_xsrf 这个键的值在茫茫无际的互联网沙漠之中指引我们用正确的姿势来登录知乎, 所以 _xsrf 可谓沙漠之舟. 如果没有 _xsrf, 我们或许有用户名和密码也无法登录知乎(我没试过, 不过我们学校的教务系统确实如此) 如上文所说, 我们在第一遍 GET 的时候可以从响应报文中的 HTML 代码里面得到这个沙漠之舟. 如下函数实现了这个功能, 返回的 str 就是 _xsrf 的值.
#+BEGIN_SRC python
import re
def getXSRF(data):
    cer = re.compile('name="_xsrf" value="(.*)"', flags = 0)
    strlist = cer.findall(data)
    return strlist[0]
#+END_SRC

*** 第四步:发射 POST !!
集齐 _xsrf, id, password 三大法宝, 我们可以发射 POST 了. 这个 POST 一旦发射过去, 我们就登陆上了服务器, 服务器就会发给我们 Cookies. 本来处理 Cookies 是个麻烦的事情, 不过 Python 的 http.cookiejar 库给了我们很方便的解决方案, 只要在创建 opener 的时候将一个 HTTPCookieProcessor 放进去, Cookies 的事情就不用我们管了. 下面的代码体现了这一点.
#+BEGIN_SRC python
import http.cookiejar
import urllib.request
def getOpener(head):
    # deal with the Cookies
    cj = http.cookiejar.CookieJar()
    pro = urllib.request.HTTPCookieProcessor(cj)
    opener = urllib.request.build_opener(pro)
    header = []
    for key, value in head.items():
        elem = (key, value)
        header.append(elem)
    opener.addheaders = header
    return opener
#+END_SRC

- 自动处理使用 opener 过程中遇到的 Cookies
- 自动在发出的 GET 或者 POST 请求中加上自定义的 Header

*** 第五步:正式运行
正式运行还差一点点, 我们要把要 POST 的数据弄成 opener.open() 支持的格式. 所以还要  urllib.parse 库里的 urlencode() 函数. 这个函数可以把 字典 或者 元组集合 类型的数据转换成 & 连接的 str.

str 还不行, 还要通过 encode() 来编码, 才能当作 opener.open() 或者 urlopen() 的 POST 数据参数来使用. 代码如下:

#+BEGIN_SRC python
url = 'http://www.zhihu.com/'
opener = getOpener(header)
op = opener.open(url)
data = op.read()
data = ungzip(data)     # 解压
_xsrf = getXSRF(data.decode())
 
url += 'login'
id = '这里填你的知乎帐号'
password = '这里填你的知乎密码'
postDict = {
        '_xsrf':_xsrf,
        'email': id,
        'password': password,
        'rememberme': 'y'
}
postData = urllib.parse.urlencode(postDict).encode()
op = opener.open(url, postData)
data = op.read()
data = ungzip(data)
print(data.decode())  # 你可以根据你的喜欢来处理抓取回来的数据了!

#+END_SRC

*** 完整代码
#+BEGIN_SRC python
  import urllib
  import http
  import gzip

  def getOpener(headers):
      cookies = http.cookiejar.CookieJar()
      opener = urllib.request.build_opener( urllib.request.HTTPCookieProcessor(cookies))
      header = []    
      for key,value in headers.items():
          elem = (key , value)
          header.append(elem)
          
      opener.addheaders(header)
      return opener

  def ungzip(data):
      try:
          print("正在解压....")
          data = gzip.decompress(data)
          print("解压完成！")
      except Exception:
          print("无需解压")
      return data

  headers = {
      'Connection': 'Keep-Alive',
      'Accept': 'text/html, application/xhtml+xml, */*',
      'Accept-Language': 'en-US,en;q=0.8,zh-Hans-CN;q=0.5,zh-Hans;q=0.3',
      'User-Agent': 'Mozilla/5.0 (Windows NT 6.3; WOW64; Trident/7.0; rv:11.0) like Gecko'
  }

  url = "www.baidu.com"

  opener = getOpener(headers)
  response = opener.open(url,timeout = 1000)
  data = response.read()
  data = ungzip(data)
  print(data)

#+END_SRC
